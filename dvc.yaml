stages:
  auth:
    cmd: calkit xenv -n main -- python scripts/auth.py
    deps:
      - scripts/auth.py
      - requirements.txt
    outs:
      - .env:
          cache: false
          persist: true
    always_changed: true
  # Before the pipeline is executed, we will update dates in params.yaml to
  # ensure this is rerun when we turn over to a new day
  get-data:
    cmd: calkit xenv -n main -- python scripts/get-data.py --end ${today}
    deps:
      - requirements.txt
      - scripts/get-data.py
      - .calkit/queues/backfill/todo.txt # New activities we missed
    outs:
      - data/activities.csv
      - .calkit/queues/backfill/done.txt
      - .calkit/queues/weather/todo.txt # Weather data we need to acquire
      - .calkit/queues/agg/todo.txt # Activities we need to aggregate
  get-weather-data:
    cmd: calkit xenv -n main -- python scripts/get-weather-data.py
    deps:
      - requirements.txt
      - scripts/get-weather-data.py
      - .calkit/queues/weather/todo.txt
    outs:
      - data/weather
      - .calkit/queues/weather/done.txt
  # TODO: Create large partitioned dataset from all streams to make it easy to
  # query
  # We will have to write out an index of dates we have processed to not
  # duplicate work?
  agg-data:
    foreach: ${months}
    do:
      cmd: calkit xenv -n main -- python scripts/agg-data.py ${item}
      deps:
        - requirements.txt
        - scripts/agg-data.py
        - .calkit/queues/agg/todo.txt
      outs:
        - data/timeseries
        - .calkit/queues/agg/done.csv
