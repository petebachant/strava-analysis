stages:
  auth:
    cmd: calkit xenv -n main -- python scripts/auth.py
    deps:
      - scripts/auth.py
      - requirements.txt
    outs:
      - .env:
          cache: false
          persist: true
    always_changed: true
  # Before the pipeline is executed, we will update dates in params.yaml to
  # ensure all the correct dates up until today are there
  get-data:
    foreach: ${dates}
    do:
      cmd: calkit xenv -n main -- python scripts/get-data.py ${item}
      deps:
        - scripts/get-data.py
        - requirements.txt
      outs:
        - data/raw/${item}
  # Alternatively, we can update a single param and have the script be
  # responsible for keeping track of the dates it has queried
  get-data-2:
    cmd: calkit xenv -n main -- python scripts/get-data.py --end ${today}
    deps:
      - requirements.txt
      - scripts/get-data.py
    outs:
      - data/raw
  # TODO: Create large partitioned dataset from all streams to make it easy to
  # query
  # We will have to write out an index of dates we have processed to not
  # duplicate work?
  agg-data:
    foreach: ${months}
    do:
      cmd: calkit xenv -n main -- python scripts/agg-data.py ${item}
      deps:
        - requirements.txt
        - scripts/agg-data.py
        - data/raw
      outs:
        - data/processed
